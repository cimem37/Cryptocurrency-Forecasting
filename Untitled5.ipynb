{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ5T8xk2e4kF01SC3jqkgV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G6mwxbmLP65",
        "outputId": "f9a80408-6ae0-4946-8fc9-d0bccc7d6dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-6d7f9d7fb744>:12: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
            "  fgi_df['date'] = pd.to_datetime(fgi_df['timestamp'], unit='s')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        date      open      high      low    close        volume  fgi_value  \\\n",
            "0 2018-02-01  10285.10  10335.00  8750.99  9224.52  33564.764311         30   \n",
            "1 2018-02-02   9224.52   9250.00  8010.02  8873.03  49971.626975         15   \n",
            "2 2018-02-03   8873.03   9473.01  8229.00  9199.96  28725.000735         40   \n",
            "3 2018-02-04   9199.96   9368.00  7930.00  8184.81  32014.308449         24   \n",
            "4 2018-02-05   8179.99   8382.80  6625.00  6939.99  63403.182579         11   \n",
            "\n",
            "  value_classification  \n",
            "0                 Fear  \n",
            "1         Extreme Fear  \n",
            "2                 Fear  \n",
            "3         Extreme Fear  \n",
            "4         Extreme Fear  \n",
            "\n",
            "✅ Merged dataset shape: (2620, 8)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === STEP 1: Load FGI from JSON API ===\n",
        "fgi_url = \"https://api.alternative.me/fng/?limit=0&format=json\"\n",
        "response = requests.get(fgi_url).json()\n",
        "fgi_df = pd.DataFrame(response['data'])\n",
        "\n",
        "# Clean and format FGI\n",
        "fgi_df['date'] = pd.to_datetime(fgi_df['timestamp'], unit='s')\n",
        "fgi_df['fgi_value'] = fgi_df['value'].astype(int)\n",
        "fgi_df = fgi_df[['date', 'fgi_value', 'value_classification']]\n",
        "fgi_df = fgi_df.sort_values('date')\n",
        "\n",
        "# === STEP 2: Fetch ALL daily OHLCV data from Binance ===\n",
        "def fetch_all_binance_ohlcv(symbol=\"BTCUSDT\", interval=\"1d\", start_date=\"2018-02-01\"):\n",
        "    url = \"https://api.binance.com/api/v3/klines\"\n",
        "    limit = 1000\n",
        "    all_data = []\n",
        "    start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"symbol\": symbol,\n",
        "            \"interval\": interval,\n",
        "            \"startTime\": start_ts,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        if not data or \"code\" in data:\n",
        "            break\n",
        "\n",
        "        all_data.extend(data)\n",
        "\n",
        "        # Move to the next window (next day after last returned)\n",
        "        last_ts = data[-1][0]\n",
        "        start_ts = last_ts + 24 * 60 * 60 * 1000\n",
        "\n",
        "        # Stop if we got fewer than 1000 (we're at the end)\n",
        "        if len(data) < limit:\n",
        "            break\n",
        "\n",
        "        time.sleep(0.4)  # avoid rate limits\n",
        "\n",
        "    return all_data\n",
        "\n",
        "btc_data = fetch_all_binance_ohlcv()\n",
        "\n",
        "# Convert to DataFrame\n",
        "btc_df = pd.DataFrame(btc_data, columns=[\n",
        "    \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
        "    \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
        "    \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
        "])\n",
        "\n",
        "btc_df['date'] = pd.to_datetime(btc_df['open_time'], unit='ms')\n",
        "btc_df = btc_df[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
        "btc_df[['open', 'high', 'low', 'close', 'volume']] = btc_df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n",
        "\n",
        "# === STEP 3: Filter and merge ===\n",
        "start_date = pd.to_datetime(\"2018-02-01\")\n",
        "btc_df = btc_df[btc_df['date'] >= start_date]\n",
        "fgi_df = fgi_df[fgi_df['date'] >= start_date]\n",
        "\n",
        "merged_df = pd.merge(btc_df, fgi_df, on='date', how='inner')\n",
        "\n",
        "# === STEP 4: Output merged dataset ===\n",
        "print(merged_df.head())\n",
        "print(f\"\\n✅ Merged dataset shape: {merged_df.shape}\")\n",
        "\n",
        "# Optional: Save to CSV\n",
        "# merged_df.to_csv(\"btc_fgi_merged.csv\", index=False)\n"
      ]
    }
  ]
}